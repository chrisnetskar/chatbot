{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import evaluate \n",
    "import torch \n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading base model \n",
    "model_id = 'microsoft/DialoGPT-medium' \n",
    "device = 'mps' #should probably be changed to mps \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side = 'right') #required by dialogpt to be right side\n",
    "if tokenizer.pad_token is None: \n",
    "    tokenizer.pad_token = tokenizer.eos_token #perhaps use other token\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset \n",
    "\n",
    "dataset = load_dataset('/Users/christoffer/Desktop/Aubay/Mini projects/chatbot_project/chatbot/') \n",
    "print(dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1): \n",
    "    print(dataset['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_list = dataset[\"train\"][0]['questions'] \n",
    "print(dict_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): \n",
    "    print(dict_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_dict_list = len(dict_list)\n",
    "question_list = []\n",
    "answer_list = []\n",
    "for i in range(len_dict_list): \n",
    "    question, answer = dict_list[i]['question'], dict_list[i]['answer']\n",
    "    #print(f\"Question: {question} Answer: {answer}\") \n",
    "    question_list.append(question)\n",
    "    answer_list.append(answer)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing dataset \n",
    "import pandas as pd \n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'question' : question_list, \n",
    "    'answer' : answer_list\n",
    "}) \n",
    "\n",
    "data.rename(columns={\"question\":\"context\", \"answer\":\"response\"}, inplace=True) \n",
    "data['input'] = data[\"context\"] + \"\" + data[\"response\"]\n",
    "\n",
    "print(data.head(3))\n",
    "print(\"Number of columns:\", len(data.columns))\n",
    "print(\"Column names:\", data.columns.tolist())\n",
    "print(\"Number of rows:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the dataset back to a huggingface Dataset object\n",
    "hf_dataset = Dataset.from_pandas(data)\n",
    "print(hf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chat with the model \n",
    "\n",
    "def chat(inp=None):\n",
    "    model_cpu = model.to(\"mps\")  \n",
    "    if inp is None:\n",
    "        inp = []  # Use an empty list if no predefined inputs are provided\n",
    "    \n",
    "    for step in range(2):  # Limit the conversation to 5 exchanges\n",
    "        if not inp:\n",
    "            # If no predefined input, take user input\n",
    "            new_user_input_ids = tokenizer.encode(input(\">> User: \") + tokenizer.eos_token, return_tensors='pt')\n",
    "        else:\n",
    "            # Use predefined input for the current step\n",
    "            print(\">> User: \", inp[step])\n",
    "            new_user_input_ids = tokenizer.encode(inp[step] + tokenizer.eos_token, return_tensors='pt')\n",
    "        \n",
    "        # Concatenate new user input with chat history (if exists)\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "        \n",
    "        # Generate a response from the model\n",
    "        chat_history_ids = model_cpu.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "        \n",
    "        # Decode and print the model's response\n",
    "        print(\">> DialogGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
    "\n",
    "# Example usage\n",
    "chat([\"Hello, how are you?\", \"What's your name?\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training pipeline \n",
    "\n",
    "def encode(examples): \n",
    "    encoded = tokenizer(examples['input'], truncation=True, padding='max_length', max_length=40, return_tensors='pt') \n",
    "    encoded['labels'] = encoded['input_ids'][:] #supervised learning \n",
    "    return encoded \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and validation set \n",
    "split_datasets = hf_dataset.train_test_split(test_size = 0.15) \n",
    "encoded_data = split_datasets.map(encode, batched=True) \n",
    "\n",
    "#train_dataset = split_datasets['train'] \n",
    "#validation_dataset = split_datasets['test']  \n",
    "#\n",
    "#print(\"Training Dataset:\")\n",
    "#print(train_dataset)\n",
    "#print(\"\\nValidation Dataset:\")\n",
    "#print(validation_dataset) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"fine_tuned_dialogpt\", \n",
    "    num_train_epochs=5, \n",
    "    per_device_train_batch_size=4, \n",
    "    per_device_eval_batch_size=2, \n",
    "    weight_decay=0.01, \n",
    "    learning_rate=2e-5, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args=training_args, \n",
    "    train_dataset=encoded_data[\"train\"], \n",
    "    eval_dataset=encoded_data[\"test\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performance measure before fine-tuning \n",
    "pre_eval_results = trainer.evaluate(encoded_data[\"test\"])\n",
    "pre_eval_predictions = trainer.predict(encoded_data[\"test\"].select(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(predictions): \n",
    "    responses = [] \n",
    "    for idx, pred in enumerate(predictions): \n",
    "        response = tokenizer.decode(np.argmax(pred, axis=-1), skip_special_tokens=True) \n",
    "        response \n",
    "        responses.append(response) \n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model and tokenizer\n",
    "model.save_pretrained('fine_tuned_dialogpt')\n",
    "tokenizer.save_pretrained('fine_tuned_dialogpt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eval_results = trainer.evaluate(encoded_data[\"test\"]) \n",
    "post_eval_predictions = trainer.predict(encoded_data[\"test\"].select(range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate_response(pre_eval_predictions.predictions) \n",
    "res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = generate_response(post_eval_predictions.predictions) \n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing responses from base model and fine-tuned model  \n",
    "\n",
    "def chat(model, tokenizer, num_turns=4):\n",
    "    model = model.to(\"cpu\")  # Move model to CPU for demonstration\n",
    "    chat_history_ids = None\n",
    "\n",
    "    print(\"Start chatting (type 'quit' to stop):\")\n",
    "    for step in range(num_turns):\n",
    "        user_input = input(\">> User: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "\n",
    "        new_user_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors='pt')\n",
    "        bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if chat_history_ids is not None else new_user_input_ids\n",
    "\n",
    "        chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "        response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "        print(\">> DialogGPT: {}\".format(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pretrained_model = pretrained_model.to(device)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "fine_tuned_model = AutoModelForCausalLM.from_pretrained(\"fine_tuned_dialogpt\")\n",
    "fine_tuned_model = fine_tuned_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatting with base model:\n",
      "Start chatting (type 'quit' to stop):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: I'm good, how are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: I'm good at customer service, but I'm not good at customer service.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: I don't know, I'm not a customer service person.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: I don't know, I'm not a customer service person.\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatting with base model:\")\n",
    "chat(pretrained_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chatting with Fine-tuned Model:\n",
      "Start chatting (type 'quit' to stop):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: I'm good, how are you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: I'm good at customer service, yes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: You can return your product by contacting our customer support team via our website. We will assist you with the return process once you have contacted our customer support team via our website.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> DialogGPT: We offer a return policy of 1 year for all products purchased through our website. Please contact our customer support team with your questions or concerns.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nChatting with Fine-tuned Model:\")\n",
    "chat(fine_tuned_model, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
